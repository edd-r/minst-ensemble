import os
import sys
import logging

import numpy as np
import pandas as pd
import kneed as kn


import sklearn.decomposition as decomp

# add project modules to the path
path_to_module = os.path.abspath(os.path.join(os.getcwd(), "..", "src/"))
sys.path.append(path_to_module)

import src.models.train_model as train_model


file_names = {"X_train": "X_train.npy",
              "X_test": "X_test.npy",
              "y_train": "y_train.npy",
              "y_test": "y_test.npy"
              }


def find_pca_knee(pca,
                  sensitivity=10,
                  ):
    """
    Applies kneedle algorithm from kneed package to find the inflexion point as a method of automatically selecting
    the optimum number of principle components

    Args:
        pca: fitted pca model class, or similar. Needs to have explained_varience_ and explained_varience_ratio variables
        sensitivity: sensitivy to the 'curveyness' of the inflexion point. Lower values are will return fewer components,
            but may find local minima if too sensitive. Larger values are less likely to find the local minima, but may
            return too many components to use.

    Returns: two kneedle KneeLocator objects, one based on expalined varience, the other based on explained varience ratio
        explained varience ratio tends to return more components

    """

    logger = logging.getLogger(__name__)

    df = pd.DataFrame({"explained_variance": pca.explained_variance_,
                       "explained_variance_ratio": pca.explained_variance_ratio_,
                       "components": [n for n in range(len(pca.explained_variance_))]}
                      )
    df["explained_variance_ratio"] = df["explained_variance_ratio"].cumsum()

    logger.info("Finding explained varience inflexion point")
    var_kneedle = kn.knee_locator.KneeLocator(x=df["components"],
                                              y=df["explained_variance"],
                                              S=sensitivity,
                                              curve="convex",
                                              direction="decreasing"
                                              )

    logger.info("Finding explained varience ratio inflexion point")
    explained_var_kneedle = kn.knee_locator.KneeLocator(x=df["components"],
                                                        y=df["explained_variance_ratio"],
                                                        S=sensitivity,
                                                        curve="concave",
                                                        direction="increasing"
                                                        )

    return var_kneedle, explained_var_kneedle


def combine_train_test_data(file_path=os.path.join(os.getcwd(), "data/processed/"),
                            file_names=file_names,
                            augmented_suffix="_augmented",
                            blurred_suffix="_blurred",
                            scaled_suffix="_scaled"
                            ):

    """
    combines previously split train and test data for decomposition.

    Args:
        file_path: path where the data is stored
        file_names: names of the files in file_path
        augmented_suffix: is the data augmented? if so set this suffix
        blurred_suffix: is the data blurred? if so set this suffix
        scaled_suffix: is the data scaled? if so set this suffix

    Returns: the combine data, the classes and labels if that data was part of the train or test split

    """

    data_dict = train_model.load_processed_data(file_path,
                                                file_names,
                                                augmented_suffix,
                                                blurred_suffix,
                                                scaled_suffix
                                                )

    # combine X and y data
    X = np.concatenate([data_dict["X_train"], data_dict["X_test"]])
    y = np.concatenate([data_dict["y_train"], data_dict["y_test"]])
    training = np.full(data_dict["y_train"].shape, "train")
    testing = np.full(data_dict["y_test"].shape, "test")
    labels = np.concatenate([training, testing])
    return X, y, labels


def save_decomp_data(X,
                     y,
                     labels,
                     suffix,
                     path_to_write=os.path.join(os.getcwd(), "data/processed/"),
                     overwrite=False
                     ):

    """
    saves the decomposed data as seperate test train split data sets
    Args:
        X: the decomposed data
        y: data classes
        labels: if each observation is in the test or train splits
        suffix: suffix to add to the files names
        path_to_write: where the data will be saved
        overwrite: set to overwrite any files

    Returns: dictionary of test train splits of X and y

    """

    logger = logging.getLogger(__name__)
    logger.info("reconstructing train/test split")
    data_dict = {"X_train": X[labels == "train"],
                 "X_test": X[labels == "test"],
                 "y_train": y[labels == "train"],
                 "y_test": y[labels == "test"]
                 }

    logging.info(f"saving to {path_to_write}")

    for key in data_dict:
        file_name = key + suffix
        file_path=os.path.join(path_to_write, file_name)
        # behaviour depends on if file exists and what the overwrite flag is set to
        if os.path.exists(file_path + ".npy") & overwrite:
            logger.warning(f"{file_path} exists, overwriting as --overwrite flag is set")
            np.save(file_path, data_dict[key])

        elif os.path.exists(file_path+".npy") & (not overwrite):
            logger.warning(f"{file_path} exists, not overwriting as --no-overwrite flag or no flag is set")
        else:
            logger.info(f"Saving {file_name} as {file_path}")
            np.save(file_path, data_dict[key])

    logger.info("done")


    return data_dict


def main(decomp_by="varience",
         model=decomp.PCA,
         file_path=os.path.join(os.getcwd(), "data/processed/"),
         file_names=file_names,
         augmented_suffix="_augmented",
         blurred_suffix="_blurred",
         scaled_suffix="_scaled",
         overwrite=False,
         ):

    """
    main pipeline to find the numeber of components and decompose the data
    Args:
        decomp_by: variable used to select the optimum components, varience for explained variance and varience_ratio
            for explained varience ratio
        model: sklearn method to use for decomposition, must have explained_varience_ and explained_varience_ratio_
            values
        file_path: path to load data from
        file_names: file names of data in file path
        augmented_suffix: load augmented data with this suffix
        blurred_suffix: load blurred data with this suffix
        scaled_suffix: load scaled data with this suffix
        overwrite: will overwrite exisiting files if set to True

    Returns:

    """

    log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    logging.basicConfig(level=logging.INFO, format=log_fmt)
    logger = logging.getLogger(__name__)
    logger.info("loading data")

    X, y, labels = combine_train_test_data(file_path,
                                           file_names,
                                           augmented_suffix,
                                           blurred_suffix,
                                           scaled_suffix
                                           )
    data_model = model()
    logger.info("fitting PCA")
    data_model.fit(X)
    logger.info("Finding varience inflexion points")
    var_knee, explained_var_knee = find_pca_knee(pca=data_model)

    logger.info(f"decomp_by is {decomp_by}")

    if decomp_by is "varience":
        logger.info("using explained varience inflexion point")
        n_components = var_knee.knee

    elif decomp_by is "varience_ratio":
        logger.info("using explained varience ratio inflexion point")
        n_components = explained_var_knee.knee
    else:
        logger.warning("value not known, using explained variance inflexion point by default")
        n_components = var_knee.knee

    logger.info(f"fitting model with {n_components} components")
    decomp_model = model(n_components=n_components)
    X_decomp = decomp_model.fit_transform(X)

    logger.info(("saving data"))
    suffix = augmented_suffix+blurred_suffix+scaled_suffix+"_decomp"
    decomp_dict = save_decomp_data(X_decomp, y, labels, suffix, overwrite=overwrite)
    logger.info("done")
    return decomp_dict



